# 数据导入脚本使用说明

本说明文档介绍如何在 **Windows 安装的 MySQL** 与 **WSL 中运行的后端** 之间，使用 `scripts/import_dataset.py` 自动化导入 ITD-2018 数据集并生成依赖表。

## 1. 运行环境

- Windows：MySQL 已创建 `chinavis` 库以及相关表（参考 `docs/数据库操作文档.md`）。
- WSL：已安装 Python 3.8+，并能访问 Windows MySQL（确保 3306 端口对 WSL 开放）。
- 数据集：`InsiderThreatData/ITD-2018 Data Set` 目录位于项目根目录（即代码中的默认相对路径）。

## 2. 安装依赖

脚本依赖 `pymysql`。在 WSL 项目根目录执行：

```bash
pip install pymysql
```

（若已经按照 `requirements.txt` 完整安装，则可跳过。）

## 3. 脚本功能概览

`scripts/import_dataset.py` 会按以下顺序处理数据：

1. 读取 `app/json_statics/ip_id.json` 与 `weblog_record_users.json`，建立用户 ID -> IP、部门编码映射。
2. 解析 `部门职位.csv`，写入 `department` 与 `link` 表（并为缺省部门自动猜测编码，对部长/组长等设置不同 level）。
3. 遍历 `InsiderThreatData/ITD-2018 Data Set/YYYY-MM-DD/*.csv`，把 `checking.csv、email.csv、weblog.csv、login.csv、tcpLog.csv` 写入对应表。
4. 在写入 `weblog` 时同步：
   - 根据域名关键字自动给出 `办公/开发/技术/娱乐/购物/招聘/搜索/赌博` 八大标签；
   - 构建 `urldomain (url -> canonical domain)` 与 `domain_tag (domain -> tag)`。
   - 基于 IP 反查用户 ID，累计 `weblog_record` 统计（`depart, id, tag, record`）。
5. 将辅助表 `urldomain`、`domain_tag` 以及聚合表 `weblog_record` 持久化。

默认所有 CSV 均按批量（2000 行/批）写入，避免一次性占用过多内存。

## 4. 运行命令

在项目根目录（WSL）执行：

```bash
python scripts/import_dataset.py \
  --db-host <Windows_IP或localhost> \
  --db-port 3306 \
  --db-user chinavis \
  --db-pass '123456' \
  --db-name chinavis \
  --truncate
```

说明：

- `--truncate`：首次导入或需要全量重建时启用，会按顺序清空 `email/weblog/login/tcpLog/checking2/department/link/urldomain/domain_tag/weblog_record`。
- `--data-root`、`--department-file`、`--ip-map-file`、`--depart-map-file` 可选更改，如将数据放到其它路径。
- `--batch-size` 可调整每批写入的行数（默认 2000）。

运行过程中脚本会输出 `[import] ...` 日志便于追踪进度。如果出错，脚本会抛异常并自动关闭连接，可修复后重新运行（建议搭配 `--truncate` 保证幂等）。

## 5. 导入完成后的检查

建议在 MySQL 中执行下列查询确认：

```sql
SELECT COUNT(*) FROM email;
SELECT COUNT(*) FROM weblog;
SELECT COUNT(*) FROM login;
SELECT COUNT(*) FROM tcpLog;
SELECT COUNT(*) FROM checking2;
SELECT COUNT(*) FROM weblog_record;
SELECT domain, tag FROM domain_tag LIMIT 20;
```

如无异常，启动后端（WSL 中 `gunicorn -b 0.0.0.0:5000 setup:app` 或 `flask run`）即可确保 `/data/<id>`、`/login/<id>`、`/tcplog/<id或IP>` 等接口命中本地数据。

若新增关键字分类，可编辑 `scripts/import_dataset.py` 中 `DomainClassifier.CATEGORY_KEYWORDS`，然后重新运行脚本（记得 `--truncate` 或单独清空 `domain_tag`/`weblog_record`）。
