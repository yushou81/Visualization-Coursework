数据库初始化

安装 MySQL 后执行：
CREATE DATABASE chinavis DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
CREATE USER 'chinavis'@'%' IDENTIFIED BY 'YourStrongPass!';
GRANT ALL PRIVILEGES ON chinavis.* TO 'chinavis'@'%';
FLUSH PRIVILEGES;
Windows 防火墙添加入站规则允许 TCP 3306；my.ini 中设置 bind-address = 0.0.0.0（或指定实际 IP）。
表结构（确保字段匹配 ORM/DAO）
每个表都使用 InnoDB、utf8mb4，以下建表语句可直接执行：

link（app/models.py (lines 6-17) 对应）：
CREATE TABLE link (
  id VARCHAR(4) PRIMARY KEY,
  ip VARCHAR(255),
  email VARCHAR(255),
  level INT,
  depart VARCHAR(255)
);
department（app/chinavis_data.py (lines 249-263)）：
CREATE TABLE department (
  id VARCHAR(4) PRIMARY KEY,
  department VARCHAR(255),
  position VARCHAR(255)
);
checking2（考勤，app/chinavis_data.py (lines 181-205)）：
CREATE TABLE checking2 (
  id VARCHAR(4),
  day DATE,
  checkin VARCHAR(20),
  checkout VARCHAR(20)
);
email（app/models.py (lines 19-33)）：
CREATE TABLE email (
  id BIGINT AUTO_INCREMENT PRIMARY KEY,
  time DATETIME,
  proto VARCHAR(32),
  sip VARCHAR(255),
  sport INT,
  dip VARCHAR(255),
  dport INT,
  sender VARCHAR(255),
  receiver TEXT,
  subject TEXT
);
weblog（app/chinavis_data.py (lines 106-178) 直接访问）：
CREATE TABLE weblog (
  id BIGINT AUTO_INCREMENT PRIMARY KEY,
  time DATETIME,
  sip VARCHAR(255),
  sport INT,
  dip VARCHAR(255),
  dport INT,
  host VARCHAR(255)
);
weblog_record（app/models.py (lines 35-43) + get_tag_count）：
CREATE TABLE weblog_record (
  uuid BIGINT AUTO_INCREMENT PRIMARY KEY,
  depart VARCHAR(255),
  id VARCHAR(4),
  tag VARCHAR(32),
  record INT
);
login（app/models.py (lines 45-57)）：
CREATE TABLE login (
  id BIGINT AUTO_INCREMENT PRIMARY KEY,
  time DATETIME,
  user VARCHAR(255),
  proto VARCHAR(32),
  sip VARCHAR(255),
  sport INT,
  dip VARCHAR(255),
  dport INT,
  state VARCHAR(32)
);
tcpLog（app/models.py (lines 59-71)）：
CREATE TABLE tcpLog (
  id BIGINT AUTO_INCREMENT PRIMARY KEY,
  stime DATETIME,
  dtime DATETIME,
  proto VARCHAR(32),
  sip VARCHAR(255),
  sport INT,
  dip VARCHAR(255),
  dport INT,
  uplink_length BIGINT,
  downlink_length BIGINT
);
urldomain（app/chinavis_data.py (lines 231-243)）：
CREATE TABLE urldomain (
  url VARCHAR(255) PRIMARY KEY,
  domain VARCHAR(255)
);
domain_tag（app/chinavis_data.py (lines 246-260)）：
CREATE TABLE domain_tag (
  domain VARCHAR(255) PRIMARY KEY,
  tag VARCHAR(32)
);
数据来源与导入

所有原始 CSV 位于 InsiderThreatData/ITD-2018 Data Set/<YYYY-MM-DD>/，字段顺序与上述表一致（在 CSV 首行可验证）。
建议在 Windows 上编写批处理/PowerShell/WSL 脚本，按日期循环导入。例如使用 LOAD DATA（记得开启 LOCAL INFILE）：
LOAD DATA LOCAL INFILE 'C:/path/2017-11-01/weblog.csv'
INTO TABLE weblog
FIELDS TERMINATED BY ',' ENCLOSED BY '"' LINES TERMINATED BY '\n'
IGNORE 1 LINES
(@time,@sip,@sport,@dip,@dport,@host)
SET time = STR_TO_DATE(@time,'%Y-%m-%d %H:%i:%s'),
    sip=@sip, sport=@sport, dip=@dip, dport=@dport, host=@host;
其它表类似，只要把 STR_TO_DATE 的格式与 CSV 匹配（email.csv 使用 '%Y/%m/%e %H:%i'）。
link 与 department 可直接从 部门职位.csv 读取一次（可用 LOAD DATA）。
weblog_record、urldomain、domain_tag 并无直接 CSV，需要你：
urldomain: 对 weblog.host 中的域名进行标准化（去掉子路径），插入 url=host, domain=<主域>。
domain_tag: 按需求把域名分类（开发、办公、娱乐、招聘、搜索、购物、赌博等）。可以参考 app/json_statics/*.json 里的 tag 值，或通过规则脚本批量标注后导入。
weblog_record: 统计每个 id（员工）在各种 tag 上的访问次数并写入此表；脚本思路：先 JOIN link 得到 id -> ip，按 domain_tag.tag 汇总 weblog，写回 weblog_record.
确保为 email.receiver 存储 ; 分隔的多收件人字符串，后端在 getreceiver() 中会按 subject 聚合（app/chinavis_data.py (lines 262-303)）。
后端配置（WSL）

在 WSL 中设置环境变量（建议使用 .env）：
export DATABASE_URL='mysql+pymysql://chinavis:YourStrongPass!@<Windows_IP>/chinavis'
export RAW_DB_URL='123...' # 如果 chinavis_data.py 改成读取 env
修改 app/__init__.py (lines 12-19) 与 app/chinavis_data.py (lines 9-10)，改为 os.environ.get(...) 读取本地连接，避免再次访问原远程库。示例：
db_uri = os.environ.get('SQLALCHEMY_DATABASE_URI', 'mysql+pymysql://...')
app.config['SQLALCHEMY_DATABASE_URI'] = db_uri
pymysql.connect(
    host=os.environ.get('RAW_DB_HOST','127.0.0.1'),
    user=os.environ.get('RAW_DB_USER','chinavis'),
    password=os.environ.get('RAW_DB_PASS','YourStrongPass!'),
    db=os.environ.get('RAW_DB_NAME','chinavis'),
    charset='utf8mb4'
)
安装依赖并启动：pip install -r requirements.txt && gunicorn -b 0.0.0.0:5000 setup:app.
接口依赖与验证清单

/users /emails /emails_full /emails_fin /weblog_record_groups/<id> /weblog_record_users/<id> 只读取 app/json_statics/*.json（app/views.py (lines 17-117)、136-170），确认这些文件存在即可。
/login/<user_id>：需求 link.ip、login 表；验证：
SELECT l.ip, COUNT(*) FROM link l JOIN login g ON g.sip = l.ip GROUP BY l.ip;
任选一个 user_id 用 curl http://localhost:5000/login/1067 应返回折线所需 JSON。
/tcplog/<id_or_ip>：需要 link, tcpLog，且 tcpLog.stime、uplink_length 等不能为空（app/views.py (lines 179-210)）。
/data/<id>（app/chinavis_data.py (lines 20-304)）：依赖表
link（getemail），
email（getsubject、getreceiver），
department（getperson_department），
checking2（getcheck_time），
weblog + urldomain + domain_tag（getdomain、getdomain_rank、get_tag_count 也用 weblog_record）。
手动执行 curl http://localhost:5000/1487 应得到完整 JSON。
github webhook（app/github.py (lines 7-25)）仅需 Git 拉权，可选。
数据校验

各 CSV 导入后，按以下 SQL 检查：
SELECT COUNT(*) FROM email; 应为 30 天邮件总量。
SELECT COUNT(*) FROM checking2 WHERE checkin='0' AND checkout='0'; 验证缺勤记录。
SELECT COUNT(DISTINCT sip) FROM weblog; 对应 ip_id.json 中 IP 数量。
SELECT tag, SUM(record) FROM weblog_record GROUP BY tag; 确保八类标签都有数据。
若 getdomain_rank 报错，检查 urldomain 是否覆盖所有 weblog.host，否则 urltodomain() 返回 NULL。
建议的自动化脚本

在 WSL 中写 Python/pandas 脚本遍历 InsiderThreatData/ITD-2018 Data Set、调用 cursor.executemany 写库；通过 sqlalchemy engine 可以一次性写 email、weblog、login、tcpLog、checking2。
同脚本中完成 urldomain、domain_tag、weblog_record 的计算，保证接口所需的所有字段都可查。
完成上述步骤后，Windows MySQL 持有完备数据，WSL 后端所有接口都会命中本地库，避免再依赖原作者的远程服务。